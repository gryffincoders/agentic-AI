# -*- coding: utf-8 -*-
"""Research Agent

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ci9PGsWfYJAlg_p-WS-Cz1ZGJetBSymo
"""

#  Cell 1: Install Required Packages
!pip install -q transformers accelerate duckduckgo-search huggingface_hub

# (optional) if on Colab, force restart runtime to make changes apply cleanly

#  Cell 2: Import Required Libraries
from duckduckgo_search import DDGS
from transformers import pipeline
from huggingface_hub import login
import torch

#  Cell 3: Login with your Hugging Face token
# Replace 'your_hf_token' with your token (visit https://huggingface.co/settings/tokens)
login(token="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")

# Cell 4: Define Search Function to Get Context
def search_web(query, num_results=5):
    with DDGS() as ddgs:
        results = ddgs.text(query, max_results=num_results)
        return [r["body"] for r in results]

#  Cell 5: Load Zephyr-7B Model (open-access LLM)
generator = pipeline(
    "text-generation",
    model="HuggingFaceH4/zephyr-7b-beta",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    max_new_tokens=300,
    do_sample=True,
    temperature=0.7,
)

#  Cell 6: Define the Research Agent
def research_agent(query):
    context = search_web(query)
    prompt = f"""You are a helpful AI assistant helping with research. Based on the following search results, summarize key insights for the query:

    Query: {query}

    Search Results:\n""" + "\n".join(context) + "\n\nAnswer:"

    result = generator(prompt, max_new_tokens=300)[0]["generated_text"]
    print(" Answer:\n", result.split("Answer:")[-1].strip())

# Cell 7: Test the Research Agent
query = "Latest advancements in quantum computing 2025"
research_agent(query)